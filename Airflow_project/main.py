from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import requests
import pandas as pd
from airflow.providers.microsoft.azure.sensors.eventhub import AzureEventHubSensor
from airflow.providers.microsoft.azure.hooks.eventhub import AzureEventHubHook
import joblib
import sqlite3


#SQLtite Database Path
DB_PATH = "/opt/airflow/data/airflow_storage.db"
TABLE_NAME = "FLIGHT_REALTIME"
# initialize EventHub
#æš‚æ—¶ç”¨çš„æ˜¯æˆ‘çš„EVENTHUB string
EVENTHUB_CONNECTION_STR = "Endpoint=sb://bigdataeventhub.servicebus.windows.net/;SharedAccessKeyName=flighteventhub;SharedAccessKey=xBbkGCvZHDeJcjcXDI05QgEYjGYEpoHol+AEhNyW+p8=;EntityPath=flight_eventhub"

EVENTHUB_NAME = "flight_eventhub"

# 1.  Ingest Data from Azure Event Hub and Store in SQLite
def ingest_from_eventhub():
    hook = AzureEventHubHook(eventhub_conn_id='azure_eventhub_default', eventhub_name=EVENTHUB_NAME)
    messages = hook.get_conn().receive()

    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute(f"""
        CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            event_time TEXT,
            message TEXT
        )
    """)

    for msg in messages:
        cursor.execute(f"INSERT INTO {TABLE_NAME} (event_time, message) VALUES (?, ?)",
                       (datetime.now(), msg.body_as_str()))

    conn.commit()
    conn.close()


# 2. data process
'''

é—®é¢˜ğŸ¤”ï¼šæˆ‘ä»¬çš„realtime dataå…¶å®å·²ç»å¾ˆå¹²å‡€ï¼Œä¸ç¡®å®šæ˜¯å¦è¿˜éœ€è¦æ¸…æ´—

'''
def preprocess_data():
    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql(
        f"SELECT * FROM {TABLE_NAME}", conn)
    '''è¿™æ˜¯ä¸ªSQLå‘½ä»¤ï¼Œé€‰å–éœ€è¦çš„åˆ—ï¼Œè¦ä¿è¯å’Œè®­ç»ƒæ¨¡å‹ç”¨çš„ç‰¹å¾ä¸€æ ·'''

    conn.close()

    df.dropna(inplace=True)
    df.to_csv('/opt/airflow/data/preprocessed_data.csv', index=False)


def fine_tune_model():
    df = pd.read_csv('/opt/airflow/data/preprocessed_data.csv')
    train_df = df.sample(frac=0.8, random_state=42)
    test_df = df.drop(train_df.index)

    X_train, y_train = train_df.drop(columns=['label']), train_df['label']

    model = joblib.load('BATCH MODEL PATH ') # è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„

    model.fit(X_train, y_train)
    joblib.dump(model, '/opt/airflow/model/classifier.pkl') #ä¿å­˜fine-tuneçš„æ¨¡å‹



#è·å–ä¸€æ¡æœ€æ–°çš„å®æ—¶æ•°æ®è¿›è¡Œé¢„æµ‹ä»–çš„å»¶è¿Ÿ
API_URL = "https://api.example.com/data"  # æ›¿æ¢æˆå®é™… API åœ°å€

'''
ğŸ¤”
é—®é¢˜: æˆ‘ä»¬finetuneçš„æ•°æ®æ¥æºå’Œè¿™ä¸ªæ•°æ®æ¥æºæ˜¯ä¸€æ ·çš„ï¼šAviationAPIï¼Œ æ¨¡å‹å·²ç»å­¦ä¹ äº†è¿™ä¸ªä¿¡æ¯ï¼Œä¼šä¸ä¼šoverfitï¼Ÿ

'''

def fetch_data_from_api():
    """ä» API è·å–æ•°æ®å¹¶å­˜å…¥ SQLite æ•°æ®åº“"""
    API_URL = "https://api.aviationstack.com/v1/flights?access_key=15cb385363a4a32008e2c3f7597e5f14&limit=100&flight_status=active&limit=1"
    # åªæ‹¿ä¸€æ¡æœ€æ–°çš„æ•°æ® æ‰€ä»¥ğŸ‘†ä¸Šé¢ä»£ç æœ€å limit=1
    HEADERS = {
        'Accept': 'application/json',
        'Accept-Version': 'v1'
    }

    response = requests.get(API_URL)
    '''
    1.ä¸ç¡®å®šæ˜¯å¦éœ€è¦å­˜åˆ°SQLiteï¼Œæˆ–è€…ç›´æ¥æ”¾å…¥æ¨¡å‹é¢„æµ‹
    2.å¦‚æœä¸ç”¨å­˜åˆ°DBï¼Œ é‚£å¯ä»¥æŠŠè¿™ä¸€æ­¥èå…¥åˆ°ä¸‹ä¸€æ­¥ predictï¼ˆï¼‰è¿›å»
    '''
    if response.status_code == 200:
        data = response.json()

        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute(f"""
            CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                event_time TEXT,
                message TEXT
            )
        """) # è¿™æ˜¯ä¸€ä¸ªSQLè¯­å¥ï¼Œéœ€è¦ä¿®æ”¹ï¼ï¼ï¼

        # å‡è®¾ API è¿”å› JSON å¯¹è±¡ï¼ŒåŒ…å« "message" å­—æ®µ
        cursor.execute(f"INSERT INTO {TABLE_NAME} (event_time, message) VALUES (?, ?)",
                       (datetime.now(), json.dumps(data)))

        conn.commit()
        conn.close()
        print("âœ… API data has been successfully stored in SQLiteï¼")
    else:
        print(f"âŒ API errorï¼Œcode: {response.status_code}")

#Make Predictions by new model and Store in SQLite
def predict():
    model = joblib.load('/opt/airflow/model/classifier.pkl')
    new_data = "æ–°çš„realtime data" # å¯èƒ½éœ€è¦é‡æ–°ç¼–å†™ä¸€ä¸ª
    pred = model.predict(new_data)
    prob = model.predict_proba(new_data)

    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    '''SQLå‘½ä»¤ï¼Œ ç”¨äºå­˜å‚¨åˆ°sqlite'''
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS predictions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp TEXT,
            prediction INTEGER,
            confidence REAL
        )
    """)
    cursor.execute("INSERT INTO predictions (timestamp, prediction, confidence) VALUES (?, ?, ?)",
                   (datetime.now(), pred[0], prob.max()))

    conn.commit()
    conn.close()


#DAG file
#ç”¨æ¥æŠŠä»¥ä¸Šå®šä¹‰çš„å‡½æ•°ï¼Œç»„æˆç®¡é“
with DAG('ml_pipeline',
         start_date=datetime(2024, 3, 25),
         schedule_interval='@daily',
         catchup=False) as dag:
    #ç¬¬ä¸€æ­¥
    task_ingest = PythonOperator(task_id='ingest_from_eventhub', python_callable=ingest_from_eventhub)
    #ç¬¬äºŒæ­¥
    task_preprocess = PythonOperator(task_id='preprocess_data', python_callable=preprocess_data)
    #ç¬¬ä¸‰æ­¥
    task_fine_tune = PythonOperator(task_id='fine_tune_model', python_callable=fine_tune_model)
    #ç¬¬å››æ­¥

    #ç¬¬å››æ­¥
    task_predict = PythonOperator(task_id='predict', python_callable=predict)

    # Define task dependencies
    task_ingest >> task_preprocess >> task_fine_tune >> task_predict